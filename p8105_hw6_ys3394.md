p8105\_hw6\_ys3394
================
Yifan Su
2020-12-07

### Problem 1

**Create a city\_state variable (e.g. “Baltimore, MD”), and a binary
variable indicating whether the homicide is solved, omit some cities,
and analyze `black` and `white` in the `victim_race` variable, besure
`victime_age` is numeric.**

``` r
# Load and clean the data
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```

<br/>

**For the city of Baltimore, MD, use the glm function to fit a logistic
regression with resolved vs unresolved as the outcome and victim age,
sex and race as predictors.**

``` r
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error), # have an interval
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

| term              |    OR | CI\_lower | CI\_upper |
| :---------------- | ----: | --------: | --------: |
| (Intercept)       | 1.363 |     0.975 |     1.907 |
| victim\_age       | 0.993 |     0.987 |     1.000 |
| victim\_raceWhite | 2.320 |     1.648 |     3.268 |
| victim\_sexMale   | 0.426 |     0.325 |     0.558 |

<br/>

**Now run glm for each of the cities in your dataset, and extract the
adjusted odds ratio (and CI) for solving homicides comparing Black
victims to white victims.**

``` r
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

<br/>

**Create a plot that shows the estimated ORs and CIs for each city.
Organize cities according to estimated OR, and comment on the plot.**

``` r
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(y = city_state, x = OR)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = CI_lower, xmax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

<img src="p8105_hw6_ys3394_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

  - This plot shows the OR of victim among male and female for each
    city, with Albuquerque has the largest OR.

  - There are only about 5 cities with OR greater than 1, while most
    cities are with OR smaller than 1.

  - Since OR is the exponential transformation of estimates, with Female
    as default, cities with low OR means crimes conducted by male are
    more difficult to be solved. <br/>

## Problem 2

**Load and clean the data for regression analysis。**

``` r
baby_df = 
  read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(babysex, bhead, blength, bwt, gaweeks) %>% 
  drop_na() %>% 
  relocate(bwt, everything()) %>% 
  mutate(
    babysex = case_when(
      babysex == "1" ~ "male",
      babysex == "2" ~ "female") %>% 
      as.factor()
  )
```

<br/>

**Fit a regression model for birthweight. Describe your modeling process
and show a plot of model residuals against fitted values**

``` r
model_fit = lm(bwt ~ gaweeks, data = baby_df)
```

``` r
resid_plot =
baby_df %>% 
  modelr::add_residuals(model_fit) %>% 
  ggplot(aes(x = gaweeks, y = resid)) + 
  geom_violin()

pred_plot =
baby_df %>% 
  modelr::add_predictions(model_fit) %>% 
  ggplot(aes(x = gaweeks, y = pred)) + 
  geom_point()

resid_plot + pred_plot
```

<img src="p8105_hw6_ys3394_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

  - In the data cleaning step, I select five covariates `babysex`,
    `bhead`, `blength`, `bwt` and `gaweeks`. Then I recode `babysex`
    variable from numeric to factor variable female and male.

  - The first model is the simplest one with only one predictor
    `gaweeks`. For this simple linear regression, I made two plots about
    residuals and predictions over fited values.

  - For residual plot, the distribution is symmetric over residual
    equals to zero, while for prediction plot, there is positive linear
    relationship between `gaweeks` and `bwt` predictions.

<br/>

**Compare the first model to two others. Make this comparison in terms
of the cross-validated prediction error**

``` r
model_main = lm(bwt ~ blength + gaweeks, data = baby_df)
model_three = lm(bwt ~ bhead*blength*babysex, data = baby_df)
```

  - 2nd Model: Using length at birth and gestational age as predictors
    (main effects only).
  - 3rd Model: Using head circumference, length, sex, and all
    interactions (including the three-way interaction) between these.

<!-- end list -->

``` r
cv_df =
  crossv_mc(baby_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    model_fit   = map(.x = train, ~lm(bwt ~ gaweeks, data = .x)),
    model_main  = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_three = map(.x = train, ~lm(bwt ~ bhead*blength*babysex, data = as_tibble(.x)))) %>% 
  mutate(
    rmse_fit = map2_dbl(.x = model_fit, .y = test, ~rmse(model = .x, data = .y)),
    rmse_main    = map2_dbl(.x = model_main, .y = test, ~rmse(model = .x, data = .y)),
    rmse_three = map2_dbl(.x = model_three, .y = test, ~rmse(model = .x, data = .y)))
    
```

``` r
cv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

<img src="p8105_hw6_ys3394_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

  - From the violin plot showing RMSE of three models, the first model
    has a much higher RMSE compared to two others, indicating that this
    model doesn’t fit well.

  - The third model containg interactions between `bhead`, `blength` and
    `babysex` may be the best model in showing relationships between
    covariabtes. <br/>

## Problem 3

**Load the weather data**

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

<br/>

**Use 5000 bootstrap samples, plot the distribution of estimates**

``` r
weather_stp =
  weather_df %>% 
  modelr::bootstrap(5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)),
    r_square = map2_dbl(.x = models, .y = strap, ~ modelr::rsquare(.x, .y)),
    results = map(models, broom::tidy)
  ) %>% 
  dplyr::select(strap_number, r_square, results) %>% 
  unnest(results)

weather_stp %>% 
  filter(term == "tmin") %>% 
  ggplot(aes(x = estimate)) + 
  geom_density()
```

<img src="p8105_hw6_ys3394_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

  - Used the boostrap with 5000 bootstrap samples, and have estimates
    for both intercept and slope.

  - This plot shows the distribution of estimates. The distribution is
    normal to some distance with a mean at about 1.04. <br/>

**Using the 5000 bootstrap estimates, identify the 2.5% and 97.5%
quantiles to provide a 95% confidence interval for r^2 and
log(β<sup>0∗β</sup>1).**

``` r
r_ci =
  weather_stp %>% 
  summarize(
    r_ci_lower = quantile(r_square, 0.025),
    r_ci_upper = quantile(r_square, 0.975)
  ) %>% 
  knitr::kable(caption = "95% CI for R square")

weather_log =
  weather_stp %>% 
  dplyr::select(strap_number, term, estimate) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_est = log(intercept*tmin))
  
  
log_ci =
  weather_log %>% 
  summarize(
    log_ci_lower = quantile(log_est, 0.025),
    log_ci_upper = quantile(log_est, 0.975)
  ) %>% 
  knitr::kable(caption = "95% CI for log(β^0∗β^1)")

r_ci
```

| r\_ci\_lower | r\_ci\_upper |
| -----------: | -----------: |
|    0.8936977 |    0.9274807 |

95% CI for R square

``` r
log_ci
```

| log\_ci\_lower | log\_ci\_upper |
| -------------: | -------------: |
|       1.966942 |       2.058528 |

95% CI for log(ß^0\*ß^1)

<br/>

**Plot the distributions of `r^2` and `log(β^0∗β^1)`**

``` r
plot_1 =
  weather_stp %>% 
  filter(term == "tmin") %>% 
  ggplot(aes(x = r_square)) + 
  geom_density() +
  labs(title = "Distribution of r^2")

plot_2 =
  weather_log %>% 
  ggplot(aes(x = log_est)) +
  geom_density() +
  labs(title = "Distribution of log(β^0∗β^1)")

plot_1 + plot_2
```

<img src="p8105_hw6_ys3394_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

  - Distributions of `r^2` and `log(β^0∗β^1)` are in the plot above.
    Since the number of the bootstrap samples is large enough, the
    distributions of them are normal according to CLT (Central Limit
    Theorem).

<br/> <br/>
